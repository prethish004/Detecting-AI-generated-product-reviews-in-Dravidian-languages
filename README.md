#  Detecting AI-Generated Product Reviews in Dravidian Languages

[![DOI](https://zenodo.org/badge/902793095.svg)](https://doi.org/10.5281/zenodo.14847233)

## ğŸš€ Introduction
The **Shared Task on Detecting AI-Generated Product Reviews in Dravidian Languages** focuses on addressing the rise of AI-generated product reviews in **Malayalam** and **Tamil**. As AI-generated content becomes more sophisticated, ensuring **authenticity in online reviews** is critical for consumer trust. 

This competition brings together **researchers and practitioners** to develop innovative models that can accurately distinguish AI-generated reviews from human-written ones. The datasets include both **AI-generated and human-authored reviews**, providing a real-world challenge for NLP experts.

## ğŸ… Results in the Shared Task
| ğŸŒ Language  | ğŸ† Rank | ğŸ“Š F1-Score |
|-------------|--------|------------|
| ğŸ‡®ğŸ‡³ **Tamil**     | ğŸ† **8**  | ğŸ¯ **0.9298**   |
| ğŸ‡®ğŸ‡³ **Malayalam** | ğŸ† **11** | ğŸ¯ **0.8797**   |

## ğŸ“Œ Evaluation Criteria
âœ”ï¸ Submissions were accepted **only through Google Forms**. *(Submission Closed)*  
âœ”ï¸ Submission format: **team_name.zip** containing TSV files for each language as `teamname_language_run.tsv`  
&nbsp;&nbsp;&nbsp;&nbsp;âœ… Example: `teamname_tamil_run.tsv`, `teamname_malayalam_run.tsv`  
âœ”ï¸ **Macro Average F1-score** used for evaluation ğŸ“ˆ  
âœ”ï¸ **Classification systemâ€™s performance** measured in terms of **F1-Score** ğŸ”  

## ğŸ“‚ Dataset & Task
Participants were challenged to build AI models capable of **differentiating between human-written and AI-generated reviews** for:
- ğŸ‡®ğŸ‡³ **Tamil Product Reviews**
- ğŸ‡®ğŸ‡³ **Malayalam Product Reviews**

### ğŸ“Š Dataset Classes
- **Human-Written Reviews**
- **AI-Generated Reviews**

## ğŸ† Model Comparison & Performance Evaluation
| Model | ğŸ‡®ğŸ‡³ Tamil Accuracy | ğŸ‡®ğŸ‡³ Malayalam Accuracy |
|--------------|----------------|----------------|
| **BERT-Base-Multilingual-Cased** | 94% | 94% |
| **TF-IDF** | 81% | 76% |
| **Count Vectorizer** | 83% | 76% |
| **XLM-RoBERTa-Large** | 96% | 94% |

## ğŸ§  Models Used & Pretraining Methods
### **Models Used**
- **BERT-Base-Multilingual-Cased**
- **XLM-RoBERTa-Large**
- **TF-IDF with Logistic Regression**
- **Count Vectorizer with SVM**

### **Pretraining Methods**
- **BERT-Base-Multilingual-Cased:** Pretrained on 104 languages with masked language modeling.
- **XLM-RoBERTa-Large:** Trained on massive multilingual datasets with self-supervised learning.
- **TF-IDF & Count Vectorizer:** Statistical feature extraction techniques for text classification.

### ğŸ“Œ **DOI Generation**
The code is archived and assigned a **DOI** through **Zenodo** for proper citation and reference.

## âœï¸ Authors
- **Prethish GA** ([ğŸ“§ Email](mailto:prethish0409@gmail.com))
- **Vasantharan K** ([ğŸ“§ Email](mailto:vasantharank.work@gmail.com))

For further inquiries, feel free to reach out! ğŸ“©

